"""
LLM Endpoint Format Verification Script
Tests that your LLM service returns the expected OpenAI-compatible format
"""

import os
import asyncio
import aiohttp
from dotenv import load_dotenv

load_dotenv()

LLM_SERVICE_URL = os.getenv("LLM_SERVICE_URL", "").strip()

async def test_llm_endpoint():
    print("üß™ Testing LLM Endpoint Format\n")
    print(f"URL: {LLM_SERVICE_URL}\n")
    
    if not LLM_SERVICE_URL:
        print("‚ùå LLM_SERVICE_URL not configured in .env")
        return False
    
    test_payload = {
        "model": os.getenv("SERVED_MODEL_NAME", "ameena"),
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Say 'test successful' in one word."}
        ],
        "temperature": 0.1,
        "max_tokens": 10
    }
    
    try:
        print("üì§ Sending test request...")
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{LLM_SERVICE_URL}/v1/chat/completions",
                json=test_payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                
                print(f"üì• Response status: {response.status}\n")
                
                if response.status != 200:
                    error_text = await response.text()
                    print(f"‚ùå Error: {error_text}\n")
                    return False
                
                data = await response.json()
                
                # Verify structure
                print("üîç Checking response structure...")
                
                if "choices" not in data:
                    print("‚ùå Missing 'choices' field")
                    return False
                print("‚úÖ Has 'choices' field")
                
                if not data["choices"]:
                    print("‚ùå 'choices' array is empty")
                    return False
                print("‚úÖ choices[0] exists")
                
                try:
                    content = data["choices"][0]["message"]["content"]
                    print(f"‚úÖ Has message.content")
                    print(f"   Response: '{content}'\n")
                except (KeyError, IndexError, TypeError) as e:
                    print(f"‚ùå Cannot access choices[0].message.content: {e}")
                    return False
                
                print("="*60)
                print("‚úÖ LLM ENDPOINT FORMAT VERIFIED!")
                print("="*60)
                print("\nYour LLM uses correct OpenAI-compatible format:")
                print("  POST /v1/chat/completions")
                print("  { choices: [{ message: { content: '...' }}] }")
                print("\nüéâ Backend will work correctly!")
                
                return True
                
    except asyncio.TimeoutError:
        print("‚è∞ Timeout - may be cold start. Retry in 10s.")
        return False
    except Exception as e:
        print(f"‚ùå Error: {type(e).__name__}: {e}")
        return False


if __name__ == "__main__":
    print("="*60)
    print("  LLM Endpoint Format Verification")
    print("="*60)
    print()
    
    success = asyncio.run(test_llm_endpoint())
    
    if not success:
        print("\n‚ö†Ô∏è  VERIFICATION FAILED")
        print("\nTroubleshooting:")
        print("1. Check LLM_SERVICE_URL in .env")
        print("2. Open URL in browser to verify it's running")
        print("3. Wait 10s for cold start, retry")
    
    exit(0 if success else 1)
